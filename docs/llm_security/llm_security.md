å¥½çš„ã€‚æˆ‘ç»™ä½ ä» **LLM Security** å¼€å§‹ï¼Œé€å±‚æ¢³ç†å¸¸è§åˆ†ç±»ä½“ç³»ã€‚

---

# ğŸ”¹ LLM Security â€“ Big Picture

## 1. Input-level Attacks (Prompt / Data Manipulation)

è¿™äº›æ”»å‡»ç›´æ¥æ”¹è¾“å…¥ï¼Œåˆ©ç”¨ LLM çš„â€œä¾èµ–ä¸Šä¸‹æ–‡â€ç‰¹æ€§ã€‚

* **Prompt Injection**

  * *Instruction Override (Jailbreak)* â†’ è¦†ç›–åŸæŒ‡ä»¤ï¼Œç»•è¿‡é™åˆ¶ã€‚
  * *Context Injection (RAG poisoning)* â†’ åœ¨æ£€ç´¢æ–‡æ¡£é‡Œæ’å…¥æ¶æ„å†…å®¹ã€‚
  * *Data Exfiltration* â†’ å¼•å¯¼æ¨¡å‹æ³„éœ²ç³»ç»Ÿ prompt æˆ–æ•æ„Ÿä¿¡æ¯ã€‚
  * *Efficiency Abuse* â†’ å¼ºè¿«æ¨¡å‹è¿‡åº¦æ€è€ƒï¼ˆOverthink, Slowdownï¼‰ã€‚
* **Adversarial Examples (perturbations)**

  * åŠ æ‰°åŠ¨è¯ã€æ‹¼å†™å˜ä½“ã€å¯¹æŠ—å™ªå£°ï¼Œè¯±å¯¼åˆ†ç±»/ç”Ÿæˆé”™è¯¯ã€‚

---

## 2. Training-level Attacks (Model Poisoning)

è¿™äº›æ”»å‡»åœ¨è®­ç»ƒæˆ–å¾®è°ƒæ•°æ®ä¸­åŸ‹æ¯’ã€‚

* **Data Poisoning**

  * åœ¨è®­ç»ƒé›†ä¸­æ’å…¥å¸¦æ ‡ç­¾çš„æ¶æ„æ ·æœ¬ï¼Œå½±å“æ¨¡å‹è¡Œä¸ºã€‚
* **Backdoors**

  * åœ¨è®­ç»ƒä¸­åŸ‹ä¸‹è§¦å‘å™¨ï¼ˆtriggerï¼‰ï¼Œè¾“å…¥ç‰¹å®šå­—ç¬¦ä¸²æ—¶æ¨¡å‹è¾“å‡ºå¼‚å¸¸ã€‚

---

## 3. Model-level Attacks (Direct Exploitation of Parameters)

è¿™é‡Œæ˜¯ç›´æ¥å¯¹æ¨¡å‹æˆ–å…¶å†…éƒ¨æƒé‡ã€æœºåˆ¶åšæ–‡ç« ã€‚

* **Model Extraction / Stealing**

  * é€šè¿‡ API è°ƒç”¨çªƒå–å‚æ•°æˆ–è’¸é¦æˆè¿‘ä¼¼æ¨¡å‹ã€‚
* **Jailbreak via Gradient / Optimization**

  * åˆ©ç”¨å¯å¾®ç»“æ„ï¼ˆæˆ– RLHF ç¼ºé™·ï¼‰æ‰¾åˆ°ç³»ç»Ÿå¼±ç‚¹ã€‚
* **Membership Inference**

  * åˆ¤æ–­æŸä¸ªæ ·æœ¬æ˜¯å¦åœ¨è®­ç»ƒæ•°æ®é‡Œ â†’ éšç§æ³„éœ²ã€‚

---

## 4. System-level Attacks (Pipeline & Deployment)

å…³æ³¨æ¨¡å‹ä½œä¸ºæœåŠ¡éƒ¨ç½²æ—¶çš„æ•´ä½“æ¼æ´ã€‚

* **RAG Pipeline Poisoning**

  * é’ˆå¯¹å‘é‡æ•°æ®åº“æˆ–æ£€ç´¢æœºåˆ¶æŠ•æ¯’ã€‚
* **Data Leakage via Logs/Cache**

  * çªƒå–æç¤ºå†å²ã€ç¼“å­˜ç»“æœã€‚
* **Abuse of Cost/Latency**

  * DoS-style attacksï¼ˆå¦‚ Overthink token æ¶ˆè€—ï¼‰ã€‚
* **API Misuse**

  * åˆ©ç”¨è°ƒç”¨é¢‘ç‡æˆ– token é™åˆ¶æ‹–å®æœåŠ¡ã€‚

---

## 5. Output-level Risks (Content & Behavior)

å…³æ³¨æ¨¡å‹ç”Ÿæˆå†…å®¹æœ¬èº«ã€‚

* **Hallucination** â†’ æ¨¡å‹ç¼–é€ é”™è¯¯ä¿¡æ¯ã€‚
* **Toxicity** â†’ è¾“å‡ºæœ‰å®³è¨€è®ºã€‚
* **Bias / Fairness Issues** â†’ å¼ºåŒ–åè§æˆ–æ­§è§†ã€‚
* **Misuse** â†’ æ”»å‡»è€…å¼•å¯¼ç”Ÿæˆæ¶æ„ä»£ç ã€è¯ˆéª—æ–‡æœ¬ã€çˆ†ç‚¸é…æ–¹ã€‚

---

# ğŸ“Œ æ€»ç»“çš„æ ‘çŠ¶è„‰ç»œ

```
LLM Security
â”œâ”€â”€ Input-level Attacks
â”‚   â”œâ”€â”€ Prompt Injection
â”‚   â”‚   â”œâ”€â”€ Instruction Override (Jailbreak)
â”‚   â”‚   â”œâ”€â”€ Context Injection (RAG poisoning)
â”‚   â”‚   â”œâ”€â”€ Data Exfiltration
â”‚   â”‚   â””â”€â”€ Efficiency Abuse (Overthink/Slowdown)
â”‚   â””â”€â”€ Adversarial Examples
â”œâ”€â”€ Training-level Attacks
â”‚   â”œâ”€â”€ Data Poisoning
â”‚   â””â”€â”€ Backdoors
â”œâ”€â”€ Model-level Attacks
â”‚   â”œâ”€â”€ Model Extraction
â”‚   â”œâ”€â”€ Jailbreak via Gradients
â”‚   â””â”€â”€ Membership Inference
â”œâ”€â”€ System-level Attacks
â”‚   â”œâ”€â”€ RAG Pipeline Poisoning
â”‚   â”œâ”€â”€ Data Leakage (Logs/Cache)
â”‚   â”œâ”€â”€ Cost/Latency Abuse
â”‚   â””â”€â”€ API Misuse
â””â”€â”€ Output-level Risks
    â”œâ”€â”€ Hallucination
    â”œâ”€â”€ Toxicity
    â”œâ”€â”€ Bias / Fairness
    â””â”€â”€ Misuse
```

---

è¦ä¸è¦æˆ‘æŠŠè¿™ä¸ª **å¤§å›¾æ™¯** ç”¨è‹±æ–‡ç‰ˆæ ‘çŠ¶å›¾ç”»å‡ºæ¥ï¼ˆåƒä½ åˆšæ‰è¦çš„ prompt injection å›¾é‚£æ ·ï¼‰ï¼Ÿ



å¥½çš„ï¼Œç»™ä½ ä¸€ä»½ **LLM Security äº”å¤§ç±»å¯¹ç…§æ€»è§ˆè¡¨**ï¼Œè¦†ç›–åˆ†ç±» â†’ å­ç±» â†’ æ ¸å¿ƒé£é™© â†’ é˜²å¾¡æ–¹æ³•ã€‚

---

# ğŸ“Š LLM Security Overview

| **Category**                                                  | **Subtypes**                                                                                                                                                                                        | **Core Risks**                                                                                                                                  | **Defenses**                                                                                                                                                          |
| ------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Input-level Attacks** (Prompt / Data Manipulation)       | - Prompt Injection (Jailbreak, Context Injection, Data Exfiltration, Efficiency Abuse, Tool Injection, Role Confusion)<br>- Adversarial Examples (word/char perturb, paraphrase, universal trigger) | - Override safety rules<br>- Leak hidden prompts / sensitive data<br>- Waste tokens (overthink / slowdown)<br>- Wrong answers from poisoned RAG | - Channel separation (system vs. user)<br>- Context sanitization & paraphrasing<br>- Detection (rules + ML)<br>- Token budget / early stopping<br>- Output auditing   |
| **2. Training-level Attacks** (Model Poisoning)               | - Data Poisoning<br>- Backdoor Insertion<br>- Trojan Models<br>- Preference poisoning (RLHF/DPO)                                                                                                    | - Hidden backdoors<br>- Targeted misclassification<br>- Training data contamination                                                             | - Data provenance & cleaning<br>- Robust training<br>- Backdoor detection (neural cleansing, pruning)<br>- Model validation & red-teaming                             |
| **3. Model-level Attacks** (Parameter & Privacy Exploitation) | - Model Extraction (stealing)<br>- Membership Inference<br>- Model Inversion<br>- Fine-tuning backdoors<br>- Gradient leakage / side-channel                                                        | - IP theft (model replication)<br>- Training data privacy leakage<br>- Hidden backdoors via fine-tune                                           | - Differential privacy (DP-SGD)<br>- Regularization<br>- Output sanitization<br>- Access control (API, fine-tune)<br>- Watermarking                                   |
| **4. System-level Attacks** (Pipeline & Deployment)           | - RAG pipeline poisoning<br>- Data leakage via logs/cache<br>- Cost/latency abuse (DoS)<br>- API misuse / tool misuse<br>- Supply chain risks                                                       | - Poisoned retrieval context<br>- Privacy leaks from logs<br>- Service degradation, cost blowup<br>- Unauthorized external calls                | - Retrieval sanitization<br>- Logging discipline (masking, cleanup)<br>- Rate limiting / quotas<br>- Tool permission isolation<br>- Supply chain verification         |
| **5. Output-level Risks** (Content & Behavior)                | - Hallucination<br>- Toxicity<br>- Bias & Fairness issues<br>- Misuse (malicious content)                                                                                                           | - Wrong/false information<br>- Harmful or offensive text<br>- Discriminatory outputs<br>- Weaponization (malware, scams)                        | - Alignment training (SFT + RLHF/DPO)<br>- Content filters<br>- Fact-checking / verifier models<br>- Uncertainty calibration (â€œI donâ€™t knowâ€)<br>- Policy enforcement |

---

ğŸ“Œ ä¸€å¥è¯æ€»ç»“ï¼š

* **Input** â†’ æ¨¡å‹è¢«è¾“å…¥ç«¯æ§åˆ¶ï¼ˆprompt æ³¨å…¥ã€å¯¹æŠ—æ ·æœ¬ï¼‰ã€‚
* **Training** â†’ æ¨¡å‹åœ¨å­¦ä¹ é˜¶æ®µè¢«æŠ•æ¯’ï¼ˆbackdoor, poisoningï¼‰ã€‚
* **Model** â†’ æ¨¡å‹è‡ªèº«è¢«çªƒå–/åˆ©ç”¨ï¼ˆå‚æ•°çªƒå–ã€éšç§æ³„éœ²ï¼‰ã€‚
* **System** â†’ éƒ¨ç½²ç®¡çº¿è¢«åˆ©ç”¨ï¼ˆRAG, ç¼“å­˜, API æ»¥ç”¨ï¼‰ã€‚
* **Output** â†’ æ¨¡å‹ç”Ÿæˆå†…å®¹å¸¦é£é™©ï¼ˆå¹»è§‰ã€æ¯’æ€§ã€åè§ã€æ»¥ç”¨ï¼‰ã€‚

---

è¦ä¸è¦æˆ‘æŠŠè¿™ä¸ªè¡¨å†æ•´ç†æˆä¸€å¼  **å›¾å½¢åŒ–å¤§å›¾ï¼ˆè‹±æ–‡æ ‡ç­¾æ ‘çŠ¶å›¾ï¼‰**ï¼Œä¸€çœ¼çœ‹åˆ°äº”å¤§ç±»+å­ç±»ï¼Ÿ
