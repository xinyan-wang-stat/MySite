# 三篇文章比较

## **1. DeepSeek-R1 (GRPO 论文)**

* **研究重点：** 如何训练出具备强推理能力的大模型。
* **方法：**

  * 提出 **DeepSeek-R1-Zero**：直接用 **GRPO 强化学习**，不依赖监督微调 (SFT)，证明了仅靠 RL 就能激发模型的推理能力。
  * 改进为 **DeepSeek-R1**：先做少量冷启动 SFT，再进行多阶段 RL + 拒绝采样，得到更稳定、更可读的推理模型。
  * **蒸馏 (Distillation)**：把大模型的推理模式迁移到小模型 (Qwen、Llama)，效果比直接在小模型上做 RL 更好。
* **主要贡献：**

  * 验证了 **纯 RL 就能学出推理**。
  * DeepSeek-R1 在 AIME、MATH-500、GPQA、Codeforces 等推理基准上性能可比 OpenAI o1。
  * 蒸馏后的小模型性能超越以往开源模型，证明大模型发现的推理模式可以有效传递。

---

## **2. Stop Overthinking: A Survey**

* **研究重点：** 系统梳理 LLM **高效推理 (Efficient Reasoning)** 的研究进展。
* **问题背景：**

  * 长链式推理 (CoT) 提升了正确率，但带来 **“过度思考 (overthinking)”**：推理过长、输出冗余、成本高。
  * 例如，回答“2+3”这种问题，有些模型会输出上千 token 的推理过程。
* **提出的分类 (研究路线图)：**

  1. **模型层面优化**：RL + 长度奖励，SFT + 长短混合 CoT 数据。
  2. **输出层面优化**：压缩推理步骤，动态控制推理深度。
  3. **输入层面优化**：基于题目难度进行路由，提示控制推理长度。
  4. **小模型与高效数据**：蒸馏、剪枝、量化。
  5. **评测基准**：如何衡量高效推理。
* **贡献：**

  * 第一篇系统性的 **高效推理综述**。
  * 总结了数十种方法，并提出分类框架 + 公共资源库。

---

## **3. On the Overthinking of o1-Like LLMs**

* **研究重点：** 实证分析 **o1 类模型 (o1, DeepSeek-R1, QwQ 等)** 的过度思考问题。
* **发现：**

  * 在简单题上往往生成冗余解答。
  * 例如：“2+3=？”这种问题，QwQ 生成 **13 个解法**，token 数比普通模型高近 20 倍。
  * 过度思考更常见于 **简单题**，而难题并没有明显冗余。
  * **90%+ 的正确答案其实在第一轮推理就已出现**，后续步骤对正确率提升极小。
* **提出指标：**

  * **结果效率 (outcome efficiency)**：额外推理是否提升了正确率？
  * **过程效率 (process efficiency)**：解答中有多少冗余步骤？
* **解决方法：**

  * 基于自训练 (self-training) 和效率指标，对输出进行精简。
  * 实验表明：在 MATH-500 上可减少 **约 50% token**，同时保持准确率不变。

---

## **对比总结**

| 方面       | DeepSeek-R1          | Stop Overthinking (综述)          | On Overthinking (实证) |
| -------- | -------------------- | ------------------------------- | -------------------- |
| **目标**   | 训练能推理的大模型            | 总结如何让推理更高效                      | 诊断并缓解过度思考            |
| **核心问题** | 如何“生出”推理             | 如何“节省”推理                        | 为什么“推理浪费”            |
| **方法论**  | RL (GRPO)、冷启动 SFT、蒸馏 | RL + 长度奖励、SFT 数据、动态推理、prompt 路由 | 效率指标 + 自训练精简         |
| **主要贡献** | 新模型 + 开源             | 分类框架 + 全面综述                     | 实证 + 指标 + 减冗余方法      |

---

## **整体关系**

* **DeepSeek-R1**：展示了如何通过 RL 和蒸馏 **训练和构建推理能力**。
* **Survey**：提供了研究全景，讨论 **如何提升推理效率**。
* **Overthinking 实证**：指出 **效率问题的根源**，并提出指标与解决方案。

三篇文章互补：**R1 提升能力 → 产生过度思考 → 综述梳理解决方法 → 实证提出指标与改进方案**。