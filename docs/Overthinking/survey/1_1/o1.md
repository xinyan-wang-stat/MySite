# 长度协调奖励 (Length-Harmonizing Reward): O1-Pruner

## 1. 回顾一下 PPO 是干嘛的

* **PPO (Proximal Policy Optimization)** 是强化学习里最常用的策略梯度算法。
* 核心思想：

  1. 让模型（policy）生成一些答案。
  2. 给这些答案打分（奖励）。
  3. 更新模型参数，让它更倾向于生成高分答案。
  4. **更新时不能离旧模型太远**，所以 PPO 有个“裁剪 (clipping)”机制，保证训练稳定。

公式大致是：

$$
L(\theta) = \mathbb{E} \Big[ \min\big(r(\theta)A, \text{clip}(r(\theta), 1-\epsilon, 1+\epsilon)A\big)\Big]
$$

其中 $r(\theta)$ 是新旧策略概率比，$A$ 是优势函数（优势越大，奖励越高）。

---

## 2. 在 O1-Pruner 里的 PPO 用法

O1-Pruner 和常规 PPO 的区别，就是**奖励函数换成了“长度协调奖励”**：

* **普通 RL**奖励 = 正确率奖励 + 格式奖励。
* **O1-Pruner**奖励 = 正确率奖励 + 格式奖励 + **长度协调奖励**。

### 奖励怎么给？

1. 模型生成答案 $y_{\text{pred}}$，参考模型生成答案 $y_{\text{ref}}$。
2. 算两者 CoT 的 token 数：$L(y_{\text{pred}}), L(y_{\text{ref}})$。
3. 如果 $y_{\text{pred}}$ 比参考更短，同时答案对 → 给额外奖励。

   * 奖励例子：$\;r_{\text{len}} = \frac{L(y_{\text{ref}})}{L(y_{\text{pred}})} - 1$。
4. 如果更长 → 扣分。
5. 如果缩短但答错 → 不加分。

这样得到的总奖励 $R$ 就用在 PPO 更新里。

---

## 3. 整个训练循环（简化版）

1. **采样**：给定一道题，模型输出推理过程和答案。
2. **打分**：用

   * **答案正确性**：对/错。
   * **格式**：有没有遵守 `<think>...<answer>`。
   * **长度协调**：和参考比，越简短越好。
     得到奖励 $R$。
3. **计算优势 (Advantage)**：用奖励减去参考基线（避免高方差）。
4. **更新 PPO**：把奖励 $R$ 塞进 PPO 的目标函数里更新参数。
5. **循环**：重复很多批次，模型逐渐学会“正确 + 简短”。

---

## 4. 打个比方

* 想象学生写作业，老师打分：

  * 答案对 → 基础分。
  * 结构对 → 加分。
  * 写得比标准答案更简洁 → 额外加分。
* PPO 就是：学生每次写完作业，老师打分，学生改进写作习惯，但不能一下子完全变样子（有个“裁剪”保护）。

---

👉 总结一句：
**O1-Pruner 用 PPO 做的是：不断采样答案 → 给“对又简短”的奖励 → 更新模型，让它既保持准确，又少啰嗦。**

---

好问题 👍。你现在有点卡在 **“奖励信号 r\_len 怎么跟 PPO 损失结合”** 这一点上。
我给你捋一下：

---

## 1. PPO 的基本结构

PPO 的优化目标（简化版）是：

$$
L(\theta) = \mathbb{E}\Big[\min\big(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon,1+\epsilon) A_t\big)\Big]
$$

* $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ → 新旧策略概率比。
* $A_t$ = **优势 (advantage)**。它就是你“希望模型更偏好的信号”。
* 优势的来源 = **奖励 R**（减去基线，做归一化）。

所以：**任何额外的奖励项（比如 r\_len）都会先加进 R，然后通过优势函数 A\_t 传进 PPO 损失**。

---

## 2. 在 O1-Pruner 里的做法

总奖励：

$$
R = w_\text{acc} \cdot r_\text{acc} \;+\; w_\text{fmt} \cdot r_\text{fmt} \;+\; w_\text{len} \cdot r_\text{len}
$$

这里的 $r_\text{len} = \frac{L_{\text{ref}}}{L_{\text{pred}}} - 1$。

然后算优势：

$$
A_t = R - b_t
$$

其中 $b_t$ 是基线（比如同 batch 奖励均值）。

最后丢进 PPO 损失里：

$$
L(\theta) = \mathbb{E}\Big[\min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon,1+\epsilon) A_t)\Big]
$$

---

## 3. 用代码口吻表示

在训练循环里：

```python
# 计算奖励
R = w_acc*r_acc + w_fmt*r_fmt + w_len*r_len  

# 转成优势（可做标准化）
A = R - baseline   # baseline 可以是 batch mean 或者 value network

# 放进 PPO 损失
ratio = (logp_new - logp_old).exp()
loss = -torch.min(ratio * A,
                  ratio.clamp(1-eps, 1+eps) * A).mean()
```

---

## 4. 总结一句

* **r\_len 不直接进损失函数**；
* 它和 r\_acc、r\_fmt 一起先组成 **奖励 R**；
* R 再变成 **优势 A**；
* A 再进入 **PPO 损失**，影响模型更新。

---

要不要我帮你画一个 **“奖励流向 PPO 损失” 的小流程图**（从 r\_len → R → A → PPO Loss）？

---

示例用一条样本，数值全给出。设：

* 题目：2+3=?
* 参考链长 $L_\text{ref}=10$；模型链长 $L_\text{pred}=6$。
* 奖励权重：$w_\text{acc}=1.0,\; w_\text{fmt}=0.1,\; w_\text{len}=0.5$。
* PPO 裁剪系数 $\epsilon=0.2$。基线 $b=0.8$。
* 旧/新策略对整条序列的对数概率：$\log p_\text{old}=-12.0$，$\log p_\text{new}=-11.5$。

### 1) 分项奖励

* 正确性：答案=5 → $r_\text{acc}=1$。
* 格式：含 `<think>...</think>` → $r_\text{fmt}=1$。
* 长度协调：$r_\text{len}= \frac{10}{6}-1 = 0.6667$。

### 2) 总奖励 $R$

$$
R = 1.0\cdot1 + 0.1\cdot1 + 0.5\cdot0.6667
  = 1 + 0.1 + 0.3333
  = 1.4333
$$

### 3) 优势 $A$

$$
A = R - b = 1.4333 - 0.8 = 0.6333
$$

### 4) PPO 比率与损失

* 概率比：$r = \exp(\log p_\text{new}-\log p_\text{old})=\exp(0.5)=1.6487$
* 裁剪后比：$\text{clip}(r, 0.8, 1.2)=1.2$
* 两项：

  * 未裁剪：$r\cdot A = 1.6487\times0.6333 = 1.044$
  * 裁剪：$1.2\times0.6333 = 0.760$
* 取最小：$\min(\cdot)=0.760$
* 损失（取负求最小）：$\text{loss}=-0.760$（优化器最小化它，相当于最大化 0.760）

---

### 反例快速对照

* **更长但正确**：若 $L_\text{pred}=14$，$r_\text{len}=10/14-1=-0.2857$，则
  $R=1+0.1+0.5\times(-0.2857)=0.9571$，$A=0.1571$
  → 更新动力显著变小。
* **答错**：常见做法是把 $r_\text{len}$ 置零，仅保留格式/惩罚：
  如 $r_\text{acc}=0,\; r_\text{fmt}=1$ → $R=0.1$，$A= -0.7$（或很小的正数，视基线），更新会压低该轨迹概率。
* **无格式**：$r_\text{fmt}=0$，会进一步降低 $R$。

可选再加 KL 正则到参考策略；但关键是：**$r_\text{len}$ 先进 $R$，再进 $A$，最后通过 PPO 的 $\min(rA,\text{clip}(\cdot)A)$ 影响梯度**。