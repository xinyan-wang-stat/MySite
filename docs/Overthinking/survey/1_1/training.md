**结论**：Training \[3] 用**策略梯度（PG）**做序列级强化学习，**只在答案正确时给奖励**，并按**长度函数惩罚**，目标是用更短的 CoT 得到同样正确率。其核心奖励：

$$
r=\mathbf{1}\{y_{\text{pred}}=y_{\text{GT}}\}\cdot\bigl(1-\alpha\,f(L(y_{\text{pred}}))\bigr),
$$

即错题零奖励，答对且越短奖励越高。 &#x20;

# 方法要点

* **思想**：长度奖励**以正确性为条件**。更短的正确推理得更高分，鼓励“短链路推理”。
* **优化器**：REINFORCE 型 PG，目标 $\mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(y|x)\, \hat R]$。
* **实现信号**：$\hat R=r$ 为上式长度惩罚奖励；$\alpha$ 控制“压缩力度”。

# 训练流程（最小闭环）

1. 采样：用当前策略 $\pi_\theta$ 对样本 $x$ 生成推理 $y_{\text{pred}}$。
2. 评测：判定是否答对 $y_{\text{GT}}$，计算 $L(y_{\text{pred}})$ 与 $r$。
3. 更新：按 PG 梯度 $\nabla_\theta \log \pi_\theta(y_{\text{pred}}|x)\, r$ 累积并反传。
4. 重复至收敛；$\alpha$ 与 $f(\cdot)$ 决定长度–准确率权衡。

# 关键设计

* **长度函数 $f(\cdot)$**：论文以抽象符号表示 $f(L)$。常见做法是对长度做归一化或单调映射，便于与 $\alpha$ 协同（此处为通用工程选择，论文未固定）。
* **只奖对的**：错误样本不给奖励，避免“为变短而变短”破坏正确性。

# 与其它长度RL的关系

* **与 PPO 系列**：PPO 等也可加入长度项或余弦/超长惩罚；Training \[3] 是**纯 PG**的最简单基线。
* **优点**：实现简单，信号清晰；**风险**：答对率低时梯度稀疏，可配合基线或混合监督缓解（通用经验）。

# 用到的数据与模型（表格记载）

* **数据集**：GSM8K、MATH-500、AIME-2024。
* **模型**：DeepSeek-R1-Distill-Qwen-1.5B、7B。

# 实操建议

* 先用 SFT 稳住**格式与可解性**，再切 PG 长度奖。
* 从小 $\alpha$ 起步，观察准确率–平均 reasoning tokens 的权衡曲线，再递增。
* 统计“正确样本的平均长度”与“总体正确率”双指标联动监控，避免过度收缩。
* 失败回退：若正确率掉幅明显，降低 $\alpha$ 或放宽 $f(\cdot)$ 的斜率。

# 一句话对比

* **Training \[3]**：PG +「对才奖、长则罚」。最简洁的“短 CoT”RL范式。

需要我把它改写成可直接跑的 PyTorch 伪代码或训练脚本吗？
