结论：**DAST** 用**偏好优化 SimPO**替代 PPO，不依赖参考模型；先**构造“短优于长”的长度偏好数据**，再做**成对偏好训练**，实现“**在对的前提下更短**”。适配 MATH‑500、AIME‑2024，常见于 DeepSeek‑R1‑Distill‑Qwen‑8B/32B 等模型。

# 1. 动机

长 CoT 易过度思考。与其在 RL 里手调长度惩罚，**不如直接教模型“短胜长”**：给同一题的两条答案，短且对的记为胜者，长的记为败者，用偏好学习优化策略。

# 2. 数据构造（关键）

* 对每个问题生成多条推理：筛出**正确**的若干条，按长度排序。
* 形成成对样本 $(y_{\text{short}}, y_{\text{long}})$：要求两条都对，且 $y_{\text{short}}$ 更短。
* 可用**长度预算** $L_{\text{budget}}$ 辅助筛选，常由“正确答案平均长度 + 最大允许长度”的线性组合定义，用来判定“过长”并构造偏好对。
* 过滤：答案错误或格式不合规的样本不入库，避免“短但错”的错误监督。

# 3. 目标函数（SimPO）

对每对样本，最大化胜者相对败者的对数似然差，带**目标间隔** $\gamma$：

$$
\mathcal{L}_{\text{SimPO}}
= \log \sigma\!\big(\beta\,[\!\underbrace{\tfrac{1}{|y_w|}\log \pi_\theta(y_w|x)}_{\text{按长度归一}}\!
-\!\underbrace{\tfrac{1}{|y_l|}\log \pi_\theta(y_l|x)}_{\text{抑制长解}}\!-\gamma]\big)
$$

其中 $\beta$ 控制温度；对数似然按长度归一，**天然偏短**；无需显式 KL 到参考模型。

# 4. 训练流程

1. 题目集上批量生成候选解；
2. 质量与格式校验 → 正确集合；
3. 构造 $(y_{\text{short}}, y_{\text{long}})$ 偏好对；
4. 最小化 $-\mathcal{L}_{\text{SimPO}}$ 更新参数；
5. 迭代：可周期性刷新样本库，保持难例覆盖。

# 5. 推理与配套

* 推理时无需改解码器。若需更省，可再叠加**早停/路由**策略。
* 可与 SFT 预热或蒸馏结合：先用 SFT 稳定格式，再用 DAST 压长度。

# 6. 对比与取舍

* 相比 **O1‑Pruner**：**不需要参考模型**，实现简单；但依赖偏好数据质量。
* 相比 **L1**：不依赖“Think for N tokens”指令，更像**直接教偏好**；泛化到不同题型更自然。
* 风险：偏好对构造不当会“过压缩”致错；用**正确性门控**和**最短下限**可缓解。

# 7. 实操参数建议

* 采样 K=8–16 条/题，取前 1–2 条最短正确链作胜者，对应一条较长正确链作败者。
* $\gamma$ 由易到难递增；$\beta$ 0.5–2 区间网格搜。
* 批次内做长度分位裁剪，避免极端超长干扰梯度。
* 指标同时看：**准确率、平均 CoT token、固定预算下的准确率曲线**。

一句话：**DAST = 用 SimPO 把“短且对”写成偏好信号，直接教会模型更简洁地思考**。
