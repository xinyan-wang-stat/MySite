## **基本思想**

* 普通推理模型（如 DeepSeek-R1、o1、QwQ）在 RL 阶段，通常奖励函数只包含：

  1. **正确率奖励**：最终答案对就+1，错就-1。
  2. **格式奖励**：比如输出必须带 `<think>…</think>`。

* 这样训练出来的模型，**往往会越训越长**（因为更长链条往往和更高正确率正相关）。

* 为了避免 **过度思考 (overthinking)**，在奖励函数里额外加入 **长度项**：

  $$
  R = R_\text{accuracy} + R_\text{format} - \alpha \cdot L
  $$

  其中 $L$ 是推理链长度，$\alpha$ 是惩罚系数。

目标是让模型在正确的前提下，倾向于输出更短、更精炼的 CoT。

---

## **方法分类与代表性工作**

### 1. **直接长度惩罚**

* **Training \[3]**：

  * 奖励 = 正确答案指示函数 − α × 预测链长度差异。
  * 短且对 → 奖励高；长且错 → 奖励低。
* **效果**：显著减少 Token 数，但容易损伤复杂题表现。

---

### 2. **长度协调奖励 (Length-Harmonizing Reward)**

* **O1-Pruner \[76]**：

  * 奖励基于“预测链长度 vs 参考模型链长度”的比值。
  * 如果比参考短但还对 → 加分；比参考长 → 扣分。
  * 公式类似：

    $$
    R = \frac{L_\text{ref}}{L_\text{pred}} - 1
    $$
  * 再加上正确率约束。
* **效果**：在 GSM8K、MATH 上减少推理长度，同时几乎不损伤准确率。

---

### 3. **余弦奖励 (Cosine Reward)**

* **Demystifying \[141]**：

  * 发现 RL 并不总是让 CoT 越长越好，有时会无意义增长。
  * 提出基于余弦函数的奖励设计：

    * 短链逐渐奖励更多，超过某个长度阈值后，奖励下降。
    * 避免训练过程中“无限拉长”。
* **效果**：使模型学会“适度”思考，而不是一味增加步骤。

---

### 4. **指令约束法 (Instruction-based Length Control)**

* **L1 \[1]**：

  * 在 Prompt 里显式加入 “Think for N tokens” 的指令。
  * 奖励函数惩罚“预测长度 vs 目标长度”的差异。
  * 本质上把长度控制转为一个可监督的条件生成任务。

---

### 5. **偏好优化 + 长度数据**

* **DAST \[98]**：

  * 构造“长链 vs 短链”偏好对，然后用 **SimPO (Simple Preference Optimization)** 训练。
  * 相当于告诉模型：短链答案 > 冗长答案。

---

### 6. **其他改进**

* **Kimi k1.5 \[109]**：在策略优化 (Policy Mirror Descent) 中引入长度惩罚，避免长链爆炸。
* **Token-Budget \[37]**：在训练中显式设定 Token 预算，把它当成约束条件。

---

## **优势与不足**

**优势**

1. **直接作用在优化目标**，效果强烈，推理长度显著缩短。
2. 灵活：可以对不同任务设计不同的长度奖励形式。
3. 理论上和正确率奖励兼容，可以做 Pareto 优化。

**不足**

1. **奖励设计复杂**：过罚会伤准确率；罚轻又没效果。
2. **算力开销大**：RL 比 SFT 成本高，且调参难度大。
3. **存在 reward hacking 风险**：模型可能学会“硬缩短”但推理质量下降。

---

## **实践建议**

1. **分任务调节权重**：数学题允许更长链；常识问答应更短。
2. **动态权重**：随着训练进展，逐渐加大长度惩罚。
3. **与 SFT 混合**：先用可变长度 CoT 数据做 SFT，再用 RL + 长度奖励精调。
4. **多指标监控**：同时看 “准确率 / 平均 Token / 延迟”，而不是只看准确率。

---

👉 总结：
**路线一的核心是通过 RL 奖励函数把“高效”直接写进目标**。不同工作提出了多种奖励设计（直接长度惩罚、长度比例、余弦奖励、指令约束、SimPO 偏好优化），它们共同目标是 **压缩不必要的推理长度**，同时维持准确率。