# Kimi k1.5 \[109]

---

## 1. 背景问题

* 普通 RL 微调推理模型时（比如 DeepSeek-R1 Zero），奖励主要看**正确率**。
* 但推理链（CoT）往往会越训越长：模型一味加步骤来保证正确 → 出现 **过度思考**。
* 需要一种办法在 **保证准确率** 的同时，**限制推理长度**。

---

## 2. Kimi k1.5 的核心做法

它引入了一种 **长度惩罚融入策略优化的机制**：

1. **优化器**：不用传统 PPO，而是采用 **Policy Mirror Descent (PMD)** 的变体。

   * PMD 是一种比 PPO 更稳定的在线策略优化方法。
2. **长度惩罚**：在目标函数里引入 **长度项**，对长 CoT 进行额外惩罚。

   * 惩罚形式类似：

     $$
     L = L_{\text{RL}} - \alpha \cdot f(\text{CoT length})
     $$
   * 其中 $f(\cdot)$ 可以是线性或非线性函数，用来控制越长扣分越多。
3. **训练方式**：在 RL 更新时，模型不仅要学会答对题，还要**在短链上得到更高奖励**。

---

## 3. 与其他方法的对比

* **O1-Pruner**：通过 “预测长度 vs 参考长度” 的比例奖励来缩短。
* **L1**：在 prompt 中加指令 “Think for N tokens”，再用 RL 奖励约束。
* **Kimi k1.5**：直接在 **优化器层面嵌入长度惩罚**，属于“策略优化算法改造”。

换句话说：

* O1-Pruner = **和参考比**
* L1 = **听指令做**
* Kimi k1.5 = **优化器里硬加长度惩罚**

---

## 4. 实验与效果

* **数据/任务**：在数学（MATH-500、AIME）、常识推理等基准上做实验。
* **发现**：

  * 过长的 CoT 会导致训练不稳定 → 在 PMD 中加长度惩罚能改善。
  * 生成长度显著减少，同时准确率保持基本不变。
* **意义**：证明了**在策略优化阶段直接施加长度约束**，是一条可行路线。

---

## 5. 总结

**Kimi k1.5 \[109] 的贡献**：

* 用 **Policy Mirror Descent + 长度惩罚**，有效缓解推理链过长问题。
* 提供了另一条路径：不是改奖励函数本身（像 O1-Pruner），也不是靠数据或指令（像 L1），而是**在 RL 优化算法内部引入长度约束**。

一句话概括：
👉 **Kimi k1.5 让 RL 优化器“自带限长功能”**，既稳住了训练，又减少了过度思考。
