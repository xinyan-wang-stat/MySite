# 指令约束法 (Instruction-based Length Control): L1

## 1. 背景问题

* 在 RL 微调推理模型时，通常直接靠奖励函数（正确性 + 长度惩罚）去“暗示”模型输出更短的 CoT。
* 但这样有两个难点：

  1. **奖励噪声大**：光靠奖励信号，模型可能不知道到底要缩到多少才合理。
  2. **题目异质性**：不同题目需要的推理长度不同，用统一的惩罚容易“过短”或“过长”。

所以 L1 提出：干脆在 **输入 prompt** 里直接告诉模型「想 N 步」，再把这个约束写进奖励。

---

## 2. 方法核心

L1 的关键就是 **“显式指令 + 长度差惩罚”**：

### (1) 数据构造

在原始问题 $x$ 后面拼接一条约束：

```
Q: (题目文本)
Instruction: Think for N tokens.
```

其中 $N$ 可以根据任务设定或随机采样。

### (2) 奖励函数

输出长度 $L(y)$，目标长度 $L(y_{GT})$。
奖励定义为：

$$
r(y, y_{GT}, L(y_{GT})) = \mathbf{1}\{y = y_{GT}\} \;-\; \alpha \cdot |L(y_{GT}) - L(y)|
$$

* 如果答案对 → 给 1 分；
* 但离目标长度差多少，就扣多少（乘权重 α）；
* 答错了 → 减去长度差，不会因为“短”就被错奖。

### (3) 优化方式

* 用 **GRPO (Group Relative Policy Optimization)** 做 RL 更新（跟 DeepSeek-R1 用的同一类方法）。
* 在优势函数里就会体现“短且对 → 高优势；长且对 → 优势变小”。

---

## 3. 实际效果

* **更可控**：因为 prompt 已经明确告诉模型“写 N 个 token”，所以模型更容易学会把推理控制在一个合理区间。
* **比单纯长度惩罚更稳定**：指令就像一个“软约束”，奖励函数再精调，这样 RL 不会乱跑。
* **适应不同题目**：可以给不同题目不同的 N（比如简单题设 N=20，难题设 N=200），模型逐渐学会“按题目调节推理长度”。

---

## 4. 和 O1-Pruner 的区别

* **O1-Pruner**：用参考模型当“标尺”，比参考更短更好。
* **L1**：直接用 prompt 明确告诉模型目标长度，并用差距来罚。
* 换句话说：

  * O1-Pruner = “跟别人比”
  * L1 = “听指令做”

---

## 5. 总结一句

**L1 方法的本质**就是：

* **在输入端用指令约束目标长度**，
* **在奖励端用“答案正确 + 长度差惩罚”**，
* 再用 RL (GRPO) 让模型学会在“对”的前提下尽量遵守长度指令。

---

要不要我给你画一个 **L1 的训练流程图**（题目+指令 → 模型输出 → 奖励计算 → PPO/GRPO 更新）？
