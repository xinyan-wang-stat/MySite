结论：**TokenSkip**是“后处理压缩”范式下的短链数据构造法。先生成完整 CoT，再按**语义重要性**评估各部分对最终答案的贡献，**删掉不重要片段**，保留关键推理，从而显著减少推理 token；随后用这些“短且对”的样本做 SFT。综述将其归入“可变长度 CoT + SFT”的代表方法之一，并给出典型数据与模型设置。

# 做法（工程化视角）

1. **长链获取**：用强模型按题目生成完整 CoT 与答案。
2. **分段与打分**：把 CoT 划分为步骤/片段，**估计每段对最终答案的语义重要性**；低重要性段标记为可删。
3. **跳步与压缩**：删除或合并低重要性段，形成更短的“精简 CoT”；关键段必须保留，以维持答案正确性。
4. **一致性校验**：压缩前后答案一致，格式合规；不一致样本丢弃。
5. **SFT 训练**：用“短且对”的样本做监督微调，作为“可变长度 CoT + SFT”管线的一部分。

# 归类与数据/模型

* **归类**：后处理式 CoT 压缩（Post‑reasoning CoT Compression）的一种，与 C3oT、Distilling 2‑1 并列。
* **数据/模型示例**：GSM8K、MATH；目标模型如 LLaMA‑3.1‑8B‑Instruct、Qwen2.5‑Instruct。

# 为什么有效

* 直接减少冗余推理 token，**保留能提升最终答案准确性的关键步骤**，压缩率高，利于高效推理。

# 使用要点

* **粒度**：以“片段/步骤”为单位打分与裁剪，比逐 token 更稳。
* **护栏**：答案一致性检查；为难题设置“最短长度下限”。
* **搭配**：可与 Self‑Training（选最短正确链）、C3oT（模型压缩器）联用，统一汇入 SFT 数据集。

# 与相邻方法对比

* **TokenSkip**：按语义重要性“跳步”，保留精简 CoT，兼顾可解释性。
* **C3oT**：由 GPT‑4 做压缩器，删冗保核，压得更狠但依赖更强老师模型。
* **Distilling 2‑1**：直接去掉推理，仅蒸馏答案，速度最快但失去显式推理。

