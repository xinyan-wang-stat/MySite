结论：**Self‑Training**=让模型自己多采样、自己挑“短且对”的解，再用 **SFT** 学会这种更高效的风格。典型做法在 o1‑like 过度思考研究与综述的“可变长度 CoT+SFT”路线里都有给出与表格化总结。

# 核心流程

1. **多样本生成**：对每道题采样 K 条推理（常见 K=10，温度≈1.0），丢弃答错样本。实验发现，同题中**最短**样本在“结果效率”和“过程效率”上往往更好。
2. **样本选择**：用于训练的正样本可取“**最短正确样本**”（Shortest）。对比研究还构造了更激进的精简版本，如 **FCS**（保留最早出现的正确解）和 **GDS** 等，均显著降 token。表 3 给出三种正样本的长度与效率统计。
3. **监督微调（SFT）**：用选出的正样本做交叉熵训练，让模型模仿“短且对”的输出。论文将此作为缓解过度思考的首选基线。
4. **对照与变体**：若走偏好学习，可以把“最短正确”当正例，将**最长**样本当负例，实测比用 greedy 负例对比信号更清晰。

# 关键细节与超参

* **采样**：K=8–16；温度 0.7–1.0；保留 `<think>/<answer>` 结构以便长度统计与格式校验。
* **筛选**：必须“答案正确 + 格式合规”；按 token 长度排序取最短；必要时加“最短下限”保护难题。（综述表 3 列出 Self‑Training 的“采 N 条再选最短”范式与使用模型。）
* **目标模型**：如 Llama‑3.2‑{1B,3B}、Llama‑3.1‑8B 等均可复现该管线。

# 评测与收益

* **效率指标**：同时看“结果效率”“过程效率”，并给出固定预算下的准确率曲线。数据表明，**最短样本**在两类效率上均优于长样本。
* **实际收益**：在 GSM8K、MATH500、GPQA、AIME 等上可减算力且不降精度，是缓解 o1‑like 过度思考的有效基线。

# 常见陷阱与对策

* **过度压缩**：仅追最短会伤复杂题。对策：混入少量长链样本或采用 FCS/GDS 这类“结构性精简”而非纯长度最短。
* **负例选择**：偏好学习中，用“最长”作负例优于 greedy 负例。
* **数据刷新**：周期性重采样，避免过拟合早期短链分布。

# 最小可行配方（落地）

* 采样 K=10 → 过滤错误与格式不合规 → 取“最短正确”或 FCS → SFT 一轮 → 评估“准确率 + 平均 CoT token + 预算曲线”。对应做法在综述“3.2 SFT with Variable‑Length CoT 数据”与表 3 中明示。

需要对比 **Shortest vs FCS vs GDS** 三种正样本在不同基准上的取舍建议吗？
