结论：**CoT‑Valve**用“参数混合”控制推理长度。先分别得到“**无推理**”与“**长推理**”两个LoRA适配器，再按系数α线性混合生成**可控长度**的推理样本，随后做逐步微调以稳步缩短CoT。适配GSM8K、PRM800k等；在QwQ‑32B‑Preview、DeepSeek‑R1‑Distill‑Llama‑8B、LLaMA‑3.1‑8B、LLaMA‑3.2‑1B、Qwen32B‑Instruct等上报告过。

# 原理

* 训练两个LoRA适配器：Δθ\_N 表示“无推理”，Δθ\_L 表示“长推理”。用它们控制是否产出CoT。
* 推理期或数据生成期做线性混合：

  $$
  \Delta\theta(\alpha)=\alpha\Delta\theta_N+(1-\alpha)\Delta\theta_L,\quad 0<\alpha<1
  $$

  α越大，越接近无推理；α越小，越接近长推理。用该混合模型生成**可变长度**CoT数据。
* **逐步微调**：在训练中逐步把 α 从1降到0，用越来越长但受控的样本做SFT，从而平滑地提升“在更短链下保持正确”的能力。

# 流程

1. 训练Δθ\_N（无CoT）与Δθ\_L（长CoT）。可用GSM8K、PRM800k等。
2. 取α混合两者，生成不同长度的推理样本，形成**可变长度数据集**。
3. 标准或渐进式SFT。渐进式策略为在微调过程中逐步调低α。

# 优点

* 长度**可控**。通过α直接调节。
* 与**在推理期获得压缩CoT**的思路一致。生成的数据更贴近模型内在知识，便于学习。
* 可与LoRA、全参SFT配合，工程迁移成本低。

# 局限与要点

* 需先拿到两个风格**分离**的适配器。若风格不纯，α的控制粒度会变差。
* α日程要平滑，防止准确率震荡。建议先在高α上稳住准确率，再逐步减小。
* 评测要同时看**准确率**与**平均CoT token**，并报告固定预算下的曲线。

# 何时用

* 你已有一个“直答”模型和一个“长CoT”模型，想要**中间态**。
* 想在**不引入RL**的前提下，用数据与参数操作实现**可控缩短**。

需要我给出一个最小实现伪代码（训练Δθ\_N/Δθ\_L→混合→SFT的PyTorch骨架）吗？
