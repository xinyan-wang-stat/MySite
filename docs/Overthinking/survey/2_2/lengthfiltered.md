**Length-filtered Vote \[123]** 是一种针对 **Chain-of-Thought (CoT) 多样性解码**的高效投票策略。它关注到：**更长的思维链并不总是带来更高准确率**，随着长度增加往往会出现错误累积，所以需要按长度筛选再投票。

---

# 主要动机

* **普通 Majority Voting (Self-Consistency)**：对生成的所有 CoT 答案直接多数投票，忽视推理质量。
* **问题**：过短的推理缺少必要步骤，过长的推理累积更多错误，都会降低最终投票可靠性。

---

# 方法核心

1. **生成多个 CoT 轨迹**（例如 BoN / 自一致性）。
2. **按推理链长度分组**：把答案划分为“短、中、长”几类。
3. **长度感知投票**：

   * 丢弃过短和过长的轨迹，只保留“中等长度”的组。
   * 在该组内再做多数投票，输出最终答案。
4. **理论支持**：作者提供了数学分析，证明在给定模型能力和任务难度下，存在一个最优的 CoT 长度，使得准确率最高。

---

# 特点

* **训练无关**：只改变推理时的投票方式，不需要额外训练。
* **效率**：比完全自一致性更节省，因为会直接丢弃低质量样本。
* **稳健性**：通过长度过滤降低了“长链错误累积”的风险。

---

# 与其它方法对比

* **Self-Consistency (SC)**：对所有样本投票；Length-filtered Vote 是其“长度感知版”，更有针对性。
* **ST-BoN (Self-Truncation Best-of-N)**：基于 embedding consistency 提前截断；而 Length-filtered Vote 是**输出后再做分组筛选**。
* **Dynasor / Certaindex**：基于置信度或进度做早停；而 Length-filtered Vote 只用**长度作为 proxy 信号**。

---

# 适用场景

* **数学/逻辑推理任务**：CoT 太短推不出结果，太长容易出错。
* **资源有限**：相比全量自一致性，Length-filtered Vote 提供更高的准确率/样本比。

---

要不要我帮你把他们的 **“最优 CoT 长度” 数学分析过程**也写成 LaTeX 公式版？这样你能直接看清楚为什么会存在最优长度。
