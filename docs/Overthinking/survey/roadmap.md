# Roadmap
结论先给：这份综述把“高效推理”路线图拆成**三条主线 + 两个配套维度**，覆盖从训练到推理再到评测的闭环。

# 主线一：模型端（训练期）

**目标**：让模型学会“少想但对”。
它强调在**训练期**就把“少想但对”这一特性内化进模型，而不是推理时再去节省。分为两条路线：

---

## 路线 1：RL + 长度奖励设计

**基本思路**

* 在常规强化学习（如 PPO、GRPO、SimPO）里，除了准确率奖励 (Accuracy Reward)、格式奖励 (Format Reward)，再加上 **长度奖励 (Length Reward)**。
* 原则：**短且正确 → 高分；长且错误 → 低分；过长 → 惩罚**。

**代表方法**

* **O1-Pruner**：PPO + “长度协调奖励”，比较预测链路和参考链路长度的比例，既缩短又保准确。
* **L1**：GRPO + “Think for N tokens” 指令，让模型在约定长度内推理。
* **Demystifying**：提出余弦函数型奖励，控制 CoT 长度增长的稳定性，避免 RL 越训越长。
* **DAST**：用 SimPO，先构造“长 vs 短”偏好数据，再做偏好优化。
* **Kimi k1.5**：长度惩罚嵌入策略镜像下降 (Policy Mirror Descent)，改善长链触发。

**优缺点**

* **优点**：直接优化目标函数，推理效率提升显著；能精确控制长度分布。
* **缺点**：

  * 奖励函数设计难，过度惩罚会损伤正确率。
  * RL 成本高，需要大量算力和样本。
  * 可能出现 reward hacking（模型学会缩短但逻辑失真）。

---

## 路线 2：SFT + 可变长度 CoT 数据

**基本思路**

* 构造 **既有长链、也有短链** 的训练数据，让模型学习到“长短都能解，但更倾向短链”。
* 关键：如何获取 **短 CoT** 数据。

**短链获取方法**

1. **采样 + 选择**：采多个推理链，挑最短的。

   * 例：Self-Training。
2. **压缩器生成**：用 GPT-4 或其他模型压缩长链成短链。

   * 例：C3oT（Concise CoT）。
3. **跳词/跳步**：按语义重要性跳过部分推理步骤。

   * 例：TokenSkip。
4. **Token 预算搜索**：预设 Token 限额，让模型在预算内完成推理。

   * 例：Token-Budget。
5. **参数合并**：把“长链模型”和“直答模型”合并，得到兼顾的中间模型。

   * 例：CoT-Valve。

**代表方法**

* **Self-Training**：在 GSM8K/MATH 上采样多条链，挑最短的做训练。
* **TokenSkip**：按语义重要性删掉不关键的 token。
* **C3oT**：用 GPT-4 把长链压缩为短链，保持逻辑关键步骤。
* **Distilling 2-1**：蒸馏时把两条长链合成一条短链。
* **Token-Budget**：显式给定预算，约束训练和推理。

**优缺点**

* **优点**：比 RL 成本低；可直接增强现有模型；方法灵活，依赖数据工程。
* **缺点**：

  * 短链数据获取有噪声，可能丢失关键信息。
  * 需要精细设计筛选和压缩策略。
  * 长链仍然有价值，完全替换会损失复杂题的表现。

---

## 总结：主线一的核心逻辑

* **RL 路径**：在优化目标函数里直接加“省字数”激励。
* **SFT 路径**：在数据层面教模型“短链也能对”。

两者是互补的：

* RL 强调 **显式约束**，适合有算力的团队；
* SFT 强调 **数据引导**，更经济，适合小规模实验；
* 实践中常见 **混合策略**：先用 SFT 打基础，再用 RL + 长度奖励精调。


# 主线二：输出端（推理期）

**目标**：在不改权重或少改权重的前提，**动态省算**。

1. **潜在表征压缩（Latent Reasoning）**

   * 核心：把显式多步思考“折叠”为少量潜在步骤或软链路，再只解码必要可见文本。
   * 代表：Coconut、SoftCoT、CODI、CCoT、Heima、Loop 等。
2. **动态推理范式（Adaptive / Test‑time Scaling）**

   * 核心：依据置信度/验证器/早停单元，**早停、剪枝、少采样**；难题再加深或多样采样。
   * 代表：Speculative‑Rejection、DPTS、ST‑BoN、More‑is‑Less、RSD、Speculative‑Thinking、LightThinker、INFTYTHINK、AdaptiveStep、Self‑Calib、PathC、RPC、ThinkDeepFast 等。
   * 信号来源：概率熵、投票一致性、外部 Verifier/PRM、路由器置信度。
   * 风险与对策：早停过度致错漏；配“再检触发”与难例回退到深推理。

# 主线三：输入端（提示与路由）

**目标**：**题目一来先控预算**。

1. **Prompt‑guided**：明确给出长度/步骤预算，或要求“简洁 CoT”，如“Think for N tokens”“Concise CoT”。

   * 代表：Token‑Budget、Chain‑of‑Draft、Token‑Complexity、CCoT、MARP、ThoughtMani、NoThinking 等。
2. **基于题目属性的路由**：按难度/置信度把“易题→直答或短链；难题→深推理/多样采样”。

   * 代表：Claude 3.7 Sonnet 的路由策略、SoT、Self‑REF、Confident、RouteLLM、THOUGHTTERMINATOR 等。
   * 风险与对策：误判难度；用校准器、代价敏感阈值、少量回看机制。

# 配套维度 A：高效数据与小模型

* **少量高效数据**驱动推理能力迁移：LIMO、s1、S2R、Light‑R1 等；与**蒸馏/剪枝/量化**结合，做“可落地的小而强”模型。
* 历史经验：从大模型蒸馏出的推理模式，往往比直接在小模型上做 RL 更稳健。

# 配套维度 B：评测与度量

* **效率度量**：结果向（是否带来额外正确）+ 过程向（为一个答案付出多少冗余步骤/Token/时间）；与实证工作提出的“Outcome/Process”指标相呼应。
* **基准**：Sys2Bench、S1‑Bench、MATH‑500、AIME、GSM8K 等，并关注“每正确样本 Token/耗时”、“在固定预算下的正确率曲线”。

# 落地决策流（建议）

1. 先上**路由 + Prompt 预算**，拿到立竿见影的成本下降。
2. 接入**动态早停/剪枝/高效采样**，对难例触发深推理通道。
3. 视资源选择：

   * 有算力：做**RL+长度奖励**微调；
   * 算力有限：做**SFT+可变长度 CoT**，并结合**蒸馏**到小模型。
4. 统一用**效率指标**做 A/B：同等正确率下最小 Token/时延为优先目标。

> 总体图景：**训练端控增长，推理端控支出，输入端控入口，数据与评测做闭环**。